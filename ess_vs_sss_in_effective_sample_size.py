# %% imports
import dill
import functools as ft
import matplotlib.pyplot as plt
import numpy as np
import time

from statsmodels.tsa import stattools

np.random.seed(1000)
# %% functions
def pi(x):
    """Calculate value of the density at the point x."""
    norm_x = np.linalg.norm(x)
    return np.exp(-norm_x - 0.5 * norm_x**2)

def pi2(x, y):
    """Basically pi(x) / pi(y)."""
    norm_x = np.linalg.norm(x)
    norm_y = np.linalg.norm(y)
    return np.exp(-norm_x + norm_y + 0.5*(-norm_x**2  + norm_y**2))

def random_MH(size=1, burn_in=0, x0=[0], print_avg_acceptance_rate=True):
    """
    Perform the Maetropolis-Hastings Random Walk algorithm w.r.t. the density pi
    "burn_in" + "size" steps starting from x0. Returns a list of sampled vectors
    after burn_in.
    """
    d = len(x0)
    sampled_values = np.zeros((size, len(x0)))
    acceptance_rates = np.zeros((burn_in + size,))

    # # noise = [x0, noise1, noise2, ...]. starting vector and then steps times a noise vector
    # noise = [np.array(x0)] + [np.random.normal(scale=0.04, size=d) for i in range(steps)]
    # # define the update function that is applied successively to the noise list
    # def update(x, y):
    #     return x + y if np.random.uniform() < pi2(x + y, x) else x
    # # apply update to noise
    # return ft.reduce(update, noise)

    for i in range(-burn_in, size):
        x1 = x0 + np.random.normal(scale=0.3, size=d)
        acceptance_rates[i] = min(1, pi2(x1, x0))
        x0 = x1 if np.random.uniform() < acceptance_rates[i] else x0
        # save values after burn_in
        if i >= 0: sampled_values[i] = x0
    if print_avg_acceptance_rate:
        print("MH: average acceptance rate =", np.mean(acceptance_rates))
    return sampled_values

def runiform_ball(d, R=1, size=1):
    """
    Sample efficiently from a uniform distribution on a d-dimensional ball
    of radius R.
    """
    if R < 0: sys.exit("ERROR in runiform_ball: R must be nonnegative")
    x = np.random.normal(size=d*size)
    u = np.random.uniform(size=size)
    # return np.array([R * u[i]**(1 / d) * x[i*d : (i+1)*d] / np.linalg.norm(x[i*d : (i+1)*d]) for i in range(size)])
    res = np.zeros((size, d))
    for i in range(size):
        x_curr = x[i*d : (i+1)*d]
        res[i] = R * u[i]**(1 / d) * x_curr / np.linalg.norm(x_curr)
    return res

def random_SSS(size=1, burn_in=0, x0=[0]):
    """
    Perform the Simple Slice Sampler algorithm w.r.t. the density pi
    "burn_in" + "size" steps starting from x0. Returns a list of sampled vectors
    after burn_in.
    """
    d = len(x0)
    sampled_values = np.zeros((size,d))
    runif = np.random.uniform(size=burn_in+size)
    runifballs = runiform_ball(d, size=burn_in+size)
    for i in range(-burn_in, size):
        t  = pi(x0) * runif[i + burn_in]
        x0 = (-1 + np.sqrt(1 - 2*np.log(t))) * runifballs[i + burn_in]
        # save values after burn_in
        if i >= 0: sampled_values[i] = x0
    return sampled_values

def random_two_segments(left_border, right_border, shift=np.pi):
    """
    Sample from a uniform distribution on a union of two swgments:
    [left_border, right_border] and [left_border + shift, right_border + shift]
    """
    if left_border > right_border:
        sys.exit("ERROR in random_two_segments: left_border must be smaller than right_border")
    x = np.random.uniform(left_border, right_border)
    return x + shift if np.random.binomial(1, 0.5) == 1 else x

def ellipse_point(x1, x2, angle):
    """Return a point on the ellipse generated by x1 and x2 with the angle a."""
    if not len(x1) == len(x2):
        sys.exit("ERROR in ellipse_point: lengths of x1 and x2 must be equal")
    return(x1 * np.cos(angle) + x2 * np.sin(angle))

def random_ESS(size=1, burn_in=0, x0=[0]):
    """
    Perform the Elliptical Slice Sampler algorithm w.r.t. the density pi
    "burn_in" + "size" steps starting from x0. Returns a list of sampled vectors
    after burn_in.
    """
    d = len(x0)
    sampled_values = np.zeros((size, d))
    runif = np.random.uniform(size=(burn_in+size)*2)
    ws = np.random.normal(size=d*(burn_in+size))
    binom = np.random.randint(0, 2, size=(burn_in+size))
    print("random numbers generated")
    for i in range(-burn_in, size):
        norm_x0 = np.linalg.norm(x0)
        t = np.exp(-norm_x0) * runif[i + burn_in]
        w = ws[(i+burn_in)*d:(i+burn_in+1)*d]
        norm_w = np.linalg.norm(w)

        Ax = norm_x0**2 - norm_w**2
        Bx = 2 * np.sum(x0 * w)
        Cx = 2 * np.log(t)**2 - norm_x0**2 - norm_w**2

        phi = np.sign(Bx) * np.arccos(Ax / np.sqrt(Ax**2 + Bx**2))
        psi = np.arccos(min(1, Cx / np.sqrt(Ax**2 + Bx**2)))

        # theta = random_two_segments((phi + psi) / 2, np.pi + (phi - psi) / 2)
        theta = np.pi * runif[i+2*burn_in+size] + (phi + psi) / 2
        theta = theta + np.pi if binom[i+burn_in] == 1 else theta

        x0 = ellipse_point(x0, w, theta)
        # save values after burn_in
        if i >= 0: sampled_values[i] = x0
    return sampled_values

def random_pCN(size=1, burn_in=0, x0=[0], print_avg_acceptance_rate=True):
    """
    Perform the Preconditioned Crank-Nicolson algorithm w.r.t. the density pi
    "burn_in" + "size" steps starting from x0. Returns a list of sampled vectors
    after burn_in.
    """
    d = len(x0)
    sampled_values = np.zeros((size, len(x0)))
    acceptance_rates = np.zeros((burn_in + size,))
    for i in range(-burn_in, size):
        norm_x0 = np.linalg.norm(x0)
        t = np.random.uniform(0, np.exp(-norm_x0))
        w = np.random.normal(size=d)

        x1 = ellipse_point(x0, w, 1.5)
        acceptance_rates[i] = min(1, np.exp(norm_x0 - np.linalg.norm(x1)))
        x0 = x1 if np.random.uniform() < acceptance_rates[i] else x0

        # save values after burn_in
        if i >= 0: sampled_values[i] = x0
    if print_avg_acceptance_rate:
        print("pCN: average acceptance rate =", np.mean(acceptance_rates))
    return sampled_values


def rho(x):
    """Basically normalized pi for d = 1."""
    return (np.exp(-abs(x) - 0.5 * x**2)) / 1.31136

def draw_histogram_check(samples, title, bins=50, range=[-3, 3]):
    """Draw histogramm with rho over it."""
    count, bins, ignored = plt.hist(samples,
                                    bins=bins, density=True, range=range)
    plt.title(title)
    # plt.plot(bins, rho(bins), color='r')
    # plt.plot(bins, np.exp(-abs(bins)) / 2, color='b')
    plt.show()

def sample_and_draw_path(algorithm, title, x0, steps):
    """
    Simulate the algorithm given number of steps starting from x0 and
    draw the path of the first coordinate.
    """
    plt.title(title)
    plt.plot(range(steps), algorithm(steps, 0, x0)[:, 0])
    plt.show()

def get_correlations(samples, k_range=range(1, 11)):
    """
    Calculate correlations of "samples" for all k in "k_range".
    Returns a vector of length len("k_range").
    """
    return [np.corrcoef(samples[:-k], samples[k:])[0, 1] for k in k_range]

# %% set initial parameters
# dimension
d = 40
# starting vector of dimension "d"
x0 = np.zeros((d,))
# number of skipped iterations at the beginning
burn_in = 10**5
# number of iterations
N = 10**6
# set the last k for calculating the autocorrelation function
k_max = 10**4
# define the test function for autocorrelation function and effective sample size
def test_func(samples):
    """Calculate log(|x|) of each row of the matrix "samples"."""
    return np.log(np.linalg.norm(samples, axis=1))

# %% set full names of the algorithms
labels = {"SSS": "Simple slice sampler",
          "ESS": "Idealized elliptical slice sampler",
          "MH" : "Metropolis-Hastings",
          "pCN": "Preconditioned Crank-Nicolson"}

# %% test algorithms
# start_time = time.time()
print("start computing")
start_time = time.time()
test_SSS = random_SSS(N, burn_in, x0)
print("SSS time: %.2f" %(time.time() - start_time))

start_time = time.time()
test_ESS = random_ESS(N, burn_in, x0)
print("ESS time: %.2f" %(time.time() - start_time))

start_time = time.time()
test_MH  = random_MH (N, burn_in, x0)
print("MH time: %.2f" %(time.time() - start_time))

start_time = time.time()
test_pCN = random_pCN(N, burn_in, x0)
print("pCN time: %.2f" %(time.time() - start_time))

# %% save the kernel state
dill.dump_session("sss_vs_ess_kernel.db")

# %% load the kernel state if needed
import dill
dill.load_session("sss_vs_ess_kernel.db")

# %% drawing one histogram of the first coordinates with the target density
values = [test_SSS[:, 0], test_ESS[:, 0], test_MH[:, 0], test_pCN[:, 0]]
count, bins, ignored = plt.hist(values, bins=20, density=True)
plt.legend(labels.keys())
plt.show()

# %% drawing separate histograms of the first coordinates with the target density
draw_histogram_check(test_SSS[:, 0], labels["SSS"])
draw_histogram_check(test_ESS[:, 0], labels["ESS"])
draw_histogram_check(test_MH [:, 0], labels["MH"])
draw_histogram_check(test_pCN[:, 0], labels["pCN"])

# %% simulating and drawing one path of the first coordinate of each algorithm
sample_and_draw_path(random_SSS, labels["SSS"], x0, 1000)
sample_and_draw_path(random_ESS, labels["ESS"], x0, 1000)
sample_and_draw_path(random_MH,  labels["MH"],  x0, 1000)
sample_and_draw_path(random_pCN, labels["pCN"], x0, 1000)

# %% calculating ACF for test_func starting from lag=1
corr_SSS = stattools.acf(test_func(test_SSS), nlags=k_max, fft=True)[1:]
corr_ESS = stattools.acf(test_func(test_ESS), nlags=k_max, fft=True)[1:]
corr_MH  = stattools.acf(test_func(test_MH ), nlags=k_max, fft=True)[1:]
corr_pCN = stattools.acf(test_func(test_pCN), nlags=k_max, fft=True)[1:]

# %% plot ACF
plt.plot(range(1, k_max + 1), corr_SSS)
plt.plot(range(1, k_max + 1), corr_ESS)
plt.plot(range(1, k_max + 1), corr_MH )
plt.plot(range(1, k_max + 1), corr_pCN)
plt.title("40 dimensional ACF for log(|x|)")
plt.xscale("log")
plt.legend(labels.values())
plt.savefig("Autocorrelation_norm.pdf")
plt.show()
